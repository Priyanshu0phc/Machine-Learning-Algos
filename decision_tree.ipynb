{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question-3\n",
    "\n",
    "Decision Tree Node Structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing a Node:\n",
    "\n",
    "Parameters:\n",
    "        - feature_index: Index of feature used for splitting (None for leaf nodes)\n",
    "        - threshold: Threshold value for splitting (None for leaf nodes)\n",
    "        - left: Left child node (None for leaf nodes)\n",
    "        - right: Right child node (None for leaf nodes)\n",
    "        - value: Predicted class value (None for internal nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self,feature_index=None, threshold=None, left=None,right=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n",
    "    def set_as_leaf(self, value):\n",
    "        (self.feature_index,self.threshold,self.right,self.left,self.value)=(None,None,None,None,value)\n",
    "        \n",
    "    def set_as_internal(self, feature_index, threshold, left, right):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = None\n",
    "\n",
    "def print_tree(node, depth=0, feature_names=None):\n",
    "    indent = \"    \"*depth\n",
    "    \n",
    "    if node.is_leaf():\n",
    "        print(f\"{indent}Leaf: Class {node.value}\")\n",
    "    else:\n",
    "        if feature_names is not None:\n",
    "            feature_names = list(feature_names)\n",
    "        if feature_names:\n",
    "            feature_name = feature_names[node.feature_index]\n",
    "        else:\n",
    "            feature_name = f\"Feature {node.feature_index}\"\n",
    "            \n",
    "        print(f\"{indent}{feature_name} <= {node.threshold}\")\n",
    "        print_tree(node.left, depth + 1, feature_names)\n",
    "        print_tree(node.right, depth + 1, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Spliting\n",
    "\n",
    "Recursively builds the decision tree.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Feature matrix\n",
    "    - y: Target labels\n",
    "    - current_depth: Current depth of the tree\n",
    "    - max_depth: Maximum allowed depth (None for no limit)\n",
    "    - min_samples_split: Minimum number of samples required to split a node\n",
    "    \n",
    "    Returns:\n",
    "    - Node: The root node of the (sub)tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X, y, current_depth=0, max_depth=None, min_samples_split=2):\n",
    "    n_samples = X.shape[0]\n",
    "    node = Node()\n",
    "    \n",
    "    # Checking stopping conditions\n",
    "    if ((len(np.unique(y)) == 1) or (max_depth is not None and current_depth >= max_depth) or (n_samples < min_samples_split)):\n",
    "        # Make this a leaf node with the majority class\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        majority_class = unique_classes[np.argmax(counts)]\n",
    "        node.set_as_leaf(majority_class)\n",
    "        return node #To be checked at end...\n",
    "    \n",
    "    #finding best split\n",
    "    best_split = find_best_split(X, y)\n",
    "    \n",
    "    # Split the data\n",
    "    X_left = X[best_split['left_indices']]\n",
    "    y_left = y[best_split['left_indices']]\n",
    "    X_right = X[best_split['right_indices']]\n",
    "    y_right = y[best_split['right_indices']]\n",
    "    \n",
    "    left_child = build_tree(X_left, y_left, current_depth + 1, max_depth, min_samples_split)\n",
    "    right_child = build_tree(X_right, y_right, current_depth + 1, max_depth, min_samples_split)\n",
    "    \n",
    "    node.set_as_internal(best_split['feature_index'], best_split['threshold'], \n",
    "                        left_child, right_child)\n",
    "    \n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impurity Matrics\n",
    "\n",
    "~Gini Index, Weighted Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y):\n",
    "    #Hard-coded for two classes..\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    n0 = np.sum(y == 0) / len(y)\n",
    "    n1 = np.sum(y == 1) / len(y)\n",
    "    return 1 - (n0**2 + n1**2)\n",
    "\n",
    "def gini_index_wt(y_left, y_right):\n",
    "    n_left = len(y_left)\n",
    "    n_right = len(y_right)\n",
    "    n_total = n_left + n_right\n",
    "    if n_total == 0:\n",
    "        return 0\n",
    "    \n",
    "    gini_left = gini_impurity(y_left)\n",
    "    gini_right = gini_impurity(y_right)\n",
    "    left_frac=n_left/n_total*gini_left\n",
    "    right_frac=n_right/n_total*gini_right\n",
    "    weighted_gini =(left_frac)+(right_frac)\n",
    "    return weighted_gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Split Find using gini index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(X, y):\n",
    "    best_split = {\n",
    "        'left_indices': None,\n",
    "        'right_indices': None,\n",
    "        'gini': float('inf'),\n",
    "        'feature_index': None,\n",
    "        'threshold': None\n",
    "    }\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    if n_samples <= 1: #Not Sufficient Samples\n",
    "        return best_split\n",
    "    \n",
    "    for feature_index in range(n_features):\n",
    "        f_val = np.unique(X[:, feature_index])\n",
    "        \n",
    "        for threshold in f_val:\n",
    "            left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "            right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "            if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                continue\n",
    "            currgini = gini_index_wt(y[left_indices], y[right_indices])\n",
    "            \n",
    "            #Updating this if found better..\n",
    "            if currgini < best_split['gini']:\n",
    "                best_split = {\n",
    "                    'left_indices': left_indices,\n",
    "                    'right_indices': right_indices,\n",
    "                    'gini': currgini,\n",
    "                    'feature_index': feature_index,\n",
    "                    'threshold': threshold,\n",
    "                }\n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Of Samples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Function:\n",
    "One for entire feature matrix\n",
    "One for single input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, X):\n",
    "    if len(X.shape) == 1:\n",
    "        X = X.reshape(1, -1)\n",
    "    \n",
    "    predictions = []\n",
    "    for sample in X:\n",
    "        predictions.append(_predict_sample(tree, sample))\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "def _predict_sample(node, sample):\n",
    "    if node.is_leaf():\n",
    "        return node.value\n",
    "    \n",
    "    if sample[node.feature_index] <= node.threshold:\n",
    "        return _predict_sample(node.left, sample)\n",
    "    else:\n",
    "        return _predict_sample(node.right, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training On DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make all features into integers:\n",
    "Income(Low:0, Medium:1,High:2)\n",
    "Student(Yes:1,NO:0)\n",
    "Credit(Fair:0,Excellent:1)\n",
    "Buy_Computer(Yes:1,No:0)\n",
    "\n",
    "Making changes to data too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X):\n",
      "[[25  2  0  0]\n",
      " [30  2  0  1]\n",
      " [35  1  0  0]\n",
      " [40  0  0  0]\n",
      " [45  0  1  0]\n",
      " [50  0  1  1]\n",
      " [55  1  1  1]\n",
      " [60  2  0  0]]\n",
      "\n",
      "Target (y): [0 0 1 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'Age': [25, 30, 35, 40, 45, 50, 55, 60],\n",
    "    'Income': ['High', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High'],\n",
    "    'Student': ['No', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No'],\n",
    "    'Credit Rating': ['Fair', 'Excellent', 'Fair', 'Fair', 'Fair', 'Excellent', 'Excellent', 'Fair'],\n",
    "    'Buy Computer': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "income_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "df['Income'] = df['Income'].map(income_mapping)\n",
    "\n",
    "student_mapping = {'No': 0, 'Yes': 1}\n",
    "df['Student'] = df['Student'].map(student_mapping)\n",
    "\n",
    "credit_mapping = {'Fair': 0, 'Excellent': 1}\n",
    "df['Credit Rating'] = df['Credit Rating'].map(credit_mapping)\n",
    "\n",
    "target_mapping = {'No': 0, 'Yes': 1}\n",
    "df['Buy Computer'] = df['Buy Computer'].map(target_mapping)\n",
    "\n",
    "X = df.drop('Buy Computer', axis=1).values\n",
    "y = df['Buy Computer'].values\n",
    "\n",
    "#prepared data\n",
    "print(\"Features (X):\")\n",
    "print(X)\n",
    "print(\"\\nTarget (y):\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict whether a new person (Age = 42, Income = Low, Student = No,\n",
    " Credit = Excellent) will buy a computer.\n",
    " Making Prediction.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Structure:\n",
      "Income <= 1\n",
      "    Age <= 45\n",
      "        Leaf: Class 1\n",
      "        Age <= 50\n",
      "            Leaf: Class 0\n",
      "            Leaf: Class 1\n",
      "    Leaf: Class 0\n",
      "Prediction for new sample: Yes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "decision_tree = build_tree(X, y, max_depth=3)\n",
    "print(\"Decision Tree Structure:\")\n",
    "\n",
    "print_tree(decision_tree, feature_names=list(df.columns[:-1])) # Exclude target column\n",
    "test_sample = np.array([42, 0, 0, 1]) #Numerical representation\n",
    "prediction = predict(decision_tree, test_sample)\n",
    "verdict = \"Yes\" if prediction[0] == 1 else \"No\"\n",
    "print(f\"Prediction for new sample: {verdict}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging: To Improve Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) BootStrap Sample building\n",
    "Create a bootstrap sample of the dataset (sampling with replacement).\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy array): Feature matrix\n",
    "    y (numpy array): Target labels\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X_sample, y_sample, oob_indices)\n",
    "        - X_sample: Bootstrap sampled feature matrix\n",
    "        - y_sample: Corresponding labels\n",
    "        - oob_indices: Indices of out-of-bag samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: (8, 4)\n",
      "\n",
      "Bootstrap sample size: (8, 4)\n",
      "Number of out-of-bag samples: 4\n",
      "Out-of-bag indices: [2 4 5 7]\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_sample(X, y):\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    oob_indices = np.setdiff1d(np.arange(n_samples), np.unique(indices))\n",
    "    \n",
    "    X_sample = X[indices]\n",
    "    y_sample = y[indices]\n",
    "    \n",
    "    return X_sample, y_sample, oob_indices\n",
    "\n",
    "X_sample, y_sample, oob_indices = bootstrap_sample(X, y)\n",
    "# Test bootstrap sampling\n",
    "print(\"Original dataset size:\", X.shape)\n",
    "print(\"\\nBootstrap sample size:\", X_sample.shape)\n",
    "print(\"Number of out-of-bag samples:\", len(oob_indices))\n",
    "print(\"Out-of-bag indices:\", oob_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Multiple Trees for bagging\n",
    "Build an ensemble of decision trees using bootstrap sampling.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy array): Feature matrix\n",
    "    y (numpy array): Target labels\n",
    "    n_trees (int): Number of trees in ensemble\n",
    "    max_depth (int): Maximum depth for each tree\n",
    "    min_samples_split (int): Minimum samples to split a node\n",
    "    Random Forest:1->for later use,0->for now where we have to consider all possible cuts.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (trees, oob_info)\n",
    "        - trees: List of decision trees\n",
    "        - oob_info: Dictionary mapping tree indices to their OOB samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bagged_trees(X, y, n_trees=10, max_depth=3, min_samples_split=2, random_forest=0):\n",
    "    trees = []\n",
    "    oob_info = {}\n",
    "    \n",
    "    for i in range(n_trees):\n",
    "        # Create a bootstrap sample\n",
    "        X_sample, y_sample, oob_indices = bootstrap_sample(X, y)\n",
    "        \n",
    "        # Build tree on bootstrap sample\n",
    "        if(random_forest==1):\n",
    "            tree = build_tree_2(X_sample, y_sample, \n",
    "                         max_depth=max_depth, \n",
    "                         min_samples_split=min_samples_split)\n",
    "        else:\n",
    "            tree = build_tree(X_sample, y_sample, \n",
    "                            max_depth=max_depth, \n",
    "                            min_samples_split=min_samples_split)\n",
    "        \n",
    "        trees.append(tree)\n",
    "        oob_info[i] = oob_indices\n",
    "    \n",
    "    return trees, oob_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOB Error Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description:\n",
    "Compute Out-of-Bag error using ensemble and OOB information.\n",
    "    Parameters:\n",
    "    ensemble: Collection of decision trees\n",
    "    oob_info: Dictionary mapping tree indices to their OOB samples\n",
    "    X: Full feature matrix\n",
    "    y: True labels\n",
    "    Returns:\n",
    "    float: OOB error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_oob_error(ensemble, oob_info, X, y):\n",
    "    n_samples = X.shape[0]\n",
    "    oob_predictions = {i: [] for i in range(n_samples)}\n",
    "    #Dict. for oob predictions.\n",
    "    for tree_idx, tree in enumerate(ensemble):\n",
    "        oob_indices = oob_info[tree_idx]\n",
    "        if len(oob_indices) > 0:\n",
    "            predictions = predict(tree, X[oob_indices])\n",
    "            # Store predictions\n",
    "            for sample_idx, pred in zip(oob_indices, predictions):\n",
    "                oob_predictions[sample_idx].append(pred)\n",
    "    \n",
    "    # computing error rate\n",
    "    errors = 0\n",
    "    valid_samples = 0\n",
    "    \n",
    "    for sample_idx in range(n_samples):\n",
    "        if oob_predictions[sample_idx]:  # If sample has OOB predictions\n",
    "            majority_vote = np.round(np.mean(oob_predictions[sample_idx]))\n",
    "            errors += (majority_vote != y[sample_idx])\n",
    "            valid_samples += 1\n",
    "    if(valid_samples==0):\n",
    "        return 0\n",
    "    error=errors/valid_samples\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4) a\n",
    "Improve the performance by bagging 10 diferent trees. Compute the OOB\n",
    " error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Bagging Prediction: Buy computer\n",
      "OOB Error Estimate: 75.000%\n"
     ]
    }
   ],
   "source": [
    "#Build ensemble with OOB tracking\n",
    "trees, oob_info = build_bagged_trees(X, y, n_trees=10)\n",
    "oob_error = compute_oob_error(trees, oob_info, X, y)\n",
    "test_sample = np.array([42, 0, 0, 1]).reshape(1, -1)\n",
    "# Get predictions from all trees\n",
    "all_predictions = np.array([predict(tree, test_sample)[0] for tree in trees])\n",
    "verdict = np.round(np.mean(all_predictions))\n",
    "action='Buy' if verdict == 1 else \"Not buy\"\n",
    "print(f\"\\nAfter Bagging Prediction: {action} computer\")\n",
    "print(f\"OOB Error Estimate: {oob_error:.3%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging With Random Feature Selection(Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build 10 decision trees, and at every node we will consider two randomly selected features for split(Without Replacement).\n",
    "Build tree methods needs to be changed, along with best split too.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_2(X, y, current_depth=0, max_depth=None, min_samples_split=2, n_random_features=2):\n",
    "    n_samples, n_features = X.shape\n",
    "    node = Node()\n",
    "    \n",
    "    # Stopping conditions\n",
    "    if ((len(np.unique(y)) == 1) or (max_depth is not None and current_depth >= max_depth) or (n_samples < min_samples_split)):\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        majority_class = unique_classes[np.argmax(counts)]\n",
    "        node.set_as_leaf(majority_class)\n",
    "        return node #Making root node\n",
    "    \n",
    "    # Random features to consider for this split\n",
    "    feature_indices = np.random.choice(n_features, size=min(n_random_features, n_features), replace=False)\n",
    "    \n",
    "    best_split = {\n",
    "        'feature_index': None,\n",
    "        'threshold': None,\n",
    "        'left_indices': None,\n",
    "        'right_indices': None,\n",
    "        'gini': float('inf')\n",
    "    }\n",
    "    \n",
    "    for feature_index in feature_indices:\n",
    "        f_val = np.unique(X[:, feature_index])\n",
    "        \n",
    "        for threshold in f_val:\n",
    "            left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "            right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "            \n",
    "            if len(left_indices)==0 or len(right_indices)==0:\n",
    "                continue\n",
    "                \n",
    "            currgini =gini_index_wt(y[left_indices], y[right_indices])\n",
    "            \n",
    "            if currgini < best_split['gini']:\n",
    "                best_split = {\n",
    "                    'feature_index':feature_index,\n",
    "                    'threshold': threshold,\n",
    "                    'left_indices': left_indices,\n",
    "                    'right_indices':right_indices,\n",
    "                    'gini': currgini\n",
    "                }\n",
    "\n",
    "    if best_split['feature_index'] is None:\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        majority_class = unique_classes[np.argmax(counts)]\n",
    "        node.set_as_leaf(majority_class)\n",
    "        return node\n",
    "    \n",
    "    X_left = X[best_split['left_indices']]\n",
    "    y_left = y[best_split['left_indices']]\n",
    "    X_right = X[best_split['right_indices']]\n",
    "    y_right = y[best_split['right_indices']]\n",
    "    \n",
    "    left_child = build_tree_2(X_left, y_left, current_depth + 1, max_depth, min_samples_split, n_random_features)\n",
    "    right_child = build_tree_2(X_right, y_right, current_depth + 1, max_depth, min_samples_split, n_random_features)\n",
    "    \n",
    "    node.set_as_internal(best_split['feature_index'], best_split['threshold'], left_child, right_child)\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve the performance by bagging 10 diferent trees but using only two\n",
    "random predictors while building the trees. Compute the OOB error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Prediction: Buy computer\n",
      "OOB Error with random feature selection: 75.000%\n"
     ]
    }
   ],
   "source": [
    "# Building ensemble of 10 trees, each using 2 random features per split\n",
    "trees, oob_info = build_bagged_trees(X, y, n_trees=10,random_forest=1)\n",
    "oob_error = compute_oob_error(trees, oob_info, X, y)\n",
    "all_predictions = np.array([predict(tree, test_sample)[0] for tree in trees])\n",
    "verdict = np.round(np.mean(all_predictions))\n",
    "action='Buy' if verdict == 1 else \"Not buy\"\n",
    "print(f\"\\nRandom Forest Prediction: {action} computer\")\n",
    "print(f\"OOB Error with random feature selection: {oob_error:.3%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
